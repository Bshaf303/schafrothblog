<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R Programming on Bryan Schafroth Portfolio</title>
    <link>/categories/r-programming/</link>
    <description>Recent content in R Programming on Bryan Schafroth Portfolio</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 26 Aug 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r-programming/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Case Study - Mobile Photo Analysis</title>
      <link>/post/2018/08/26/case-study-mobile-photo-analysis/</link>
      <pubDate>Sun, 26 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/26/case-study-mobile-photo-analysis/</guid>
      <description>Case StudyCan a mobile phone application identify a species of a plant by photo analysis?
This analysis is hypothetical and analyzes the feasibility of developing a mobile application for mobile phones (that photograph and store pictures) and identify flower pictures stored in memory. This exercise is a small part of a project and will only focus on the exploratory elements of a sample dataset called iris.
For a successful application, the algorithm shall correctly identify a flower species from a photo on the user’s phone using an image recognition model.</description>
    </item>
    
    <item>
      <title>Different Graphs for Plotting Data in R</title>
      <link>/post/2018/08/19/different-graphs-for-plotting-data-in-r/</link>
      <pubDate>Sun, 19 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/19/different-graphs-for-plotting-data-in-r/</guid>
      <description>R Graph Plotting SystemThis brief analysis demonstrates the quick ways to look at data by plotting the data points using R and ggplot2. There are four small datasets used in displaying the individual data characteristics. The point is fast and simplistic plot to reveal the represented data.
Load the Librarieslibrary(datasets)library(tidyverse)Air Quality Plotwith(airquality, {plot(Temp, Ozone, pch=19, col=&amp;quot;grey&amp;quot;, main = &amp;quot;Base R - Ozone and Temperature&amp;quot;)lines(loess.</description>
    </item>
    
    <item>
      <title>K-Means Clustering Analysis</title>
      <link>/post/2018/08/12/k-means-clustering-analysis/</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/12/k-means-clustering-analysis/</guid>
      <description>K-Means ClusteringThe first section uses the iris data set and k-means clustering starting with 3 cluster centers and an nstart of 15. The results are displayed below in the fit object. There are 3 clusters sized 50, 62, and 38. The within sum of squares by cluster is 88.4% and the smaller number (15.15, 39.82, &amp;amp; 23.87) indicates how closely related objects are in the clusters. The first cluster has the most related objects and the second cluster has the lesser of related objects mostly taken from the third cluster.</description>
    </item>
    
    <item>
      <title>Hierarchical Clustering (HCA)</title>
      <link>/post/2018/08/04/hierarchical-clustering-hca/</link>
      <pubDate>Sat, 04 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/08/04/hierarchical-clustering-hca/</guid>
      <description>Hierarchical ClusteringThe hierarchical agglomerative clustering starts with each observation as a cluster and pairs two at a time until all clusters are merged into one single cluster. The number of clusters is not known or specified in advance. The distance between all pairs of points in the data is recorded. There is a dendrogram (upside down tree structure) that is the output of the grouping of clusters. It represents and shows how many clusters were found in the data.</description>
    </item>
    
    <item>
      <title>R Analysis - Plotting Graphics</title>
      <link>/post/2018/07/29/r-analysis-plotting-graphics/</link>
      <pubDate>Sun, 29 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/29/r-analysis-plotting-graphics/</guid>
      <description>R Graphics for Exploring DataThe graphics device prints the graphics to screen or into a file type read by other apps. The screen device is on our computers. For windows its windows(), mac its quartz() and Linux its x11(). I am assuming this is built into the R functions plot(), xyplot(), and qplot() when we use them to plot. The other “devices” are the file types that are created by the device.</description>
    </item>
    
    <item>
      <title>Data Analysis - Exploratory Graphics</title>
      <link>/post/2018/07/22/data-analysis-exploratory-graphics/</link>
      <pubDate>Sun, 22 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/22/data-analysis-exploratory-graphics/</guid>
      <description>Exploratory Graphics in RThe focus of this analysis is on the ways to graph data set variables. The first section uses the EPA data set retrieved from: https://github.com/jtleek/modules/blob/master/04_ExploratoryAnalysis/exploratoryGraphs/data/avgpm25.csv the data was imported into a csv file and then used for this first section. Code notes for explaination.
The first part uses the RStudio base functionality to plot graphics then we use maps, lattice, and ggplot2 to compare and contrast.</description>
    </item>
    
    <item>
      <title>Exploratory Data Analysis - EPA Ozone Data</title>
      <link>/post/2018/07/14/exploratory-data-analysis-epa-ozone-data/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/14/exploratory-data-analysis-epa-ozone-data/</guid>
      <description>IntroductionThis exploratory data analysis will use the “hourly_44201_2014” data set obtained from the web site: https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw The data is “Criteria Gases” and labeled “Ozone (44201)” for this particular data set. The data is loaded into RStudio and shows 9,060,694 rows and 24 variables.
There are 24 variables in this data set. Looking through the variables, we can see the first four are coded presumably by the EPA. The Latitude, Longitude, and Datum are geo-location data.</description>
    </item>
    
    <item>
      <title>Data Exploration - The Chicago Dataset</title>
      <link>/post/2018/07/07/data-exploration-the-chicago-dataset/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/07/data-exploration-the-chicago-dataset/</guid>
      <description>IntroductionThis article is about using the tidyverse package in R to explore data. The Chicago dataset is the basis for this analysis. The data is air quality measurements taken over 19 years. The focus is on using the dplyr package to do the analysis. The dplyr package is part of the tidyverse in R and a suite of tools to analyze, transform, plot, and manipulate data. The base version of R does all of this; however, dplyr will work faster and cost fewer computing resources when used on big data.</description>
    </item>
    
    <item>
      <title>Simple data exploration with base R functions</title>
      <link>/post/2018/07/02/simple-data-exploration-with-base-r-functions/</link>
      <pubDate>Mon, 02 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/02/simple-data-exploration-with-base-r-functions/</guid>
      <description>Exploring the data with base R functionsThe purpose of this post is to show the basic fucntions in R to start a data analysis project. I look at the iris dataset that comes with Rstudio shown below.This briefly covers head(), str(), attributes(), summary(), dim(), names(), indexing [], table(), and plot() functions that come with base R. Simple to use.
Load the iris datasetdata(&amp;quot;iris&amp;quot;) head()Look at the iris dataset with the head() function and lists the first five rows.</description>
    </item>
    
    <item>
      <title>Reinforcement Learning Problem</title>
      <link>/post/2018/07/01/reinforcement-learing-problem/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/07/01/reinforcement-learing-problem/</guid>
      <description>Reinforcement Learning - Grid World ProblemThe first section looks at the grid world concept/problem. The environment is a 3x4 grid and the goal is to move from the start xy (1,1) (called a “state”&#34;) location to the opposite corner (4,3) called the goal state. The actions are up, down, left, right. We have the rewards of +1 &amp;amp; -1. We want the agent to find the shortest sequence of actions to get from start to end (goal).</description>
    </item>
    
    <item>
      <title>K-Means - Hierarchial Cluster Analysis</title>
      <link>/post/2018/06/17/k-means-hierarchial-cluster-analysis/</link>
      <pubDate>Sun, 17 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/06/17/k-means-hierarchial-cluster-analysis/</guid>
      <description>K-Means AnalysisThis project will use the Wholesale customer Data Set from: https://archive.ics.uci.edu/ml/datasets/Wholesale+customers. The data has 440 orbs and 8 variables. The data has the Channel and Region as integers but they are categorical in nature with 2 channels and 3 regions. We think this may need to be taken out of the analysis but will leave them in for now. A general observation about the variables there are outliers in the 6 major variables: fresh, milk, grocery, frozen, detergents_paper, delicassen.</description>
    </item>
    
    <item>
      <title>Artificial Neural Network and Support Vector Machine Analysis</title>
      <link>/post/2018/06/10/artificial-neural-network-and-support-vector-machine-analysis/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/06/10/artificial-neural-network-and-support-vector-machine-analysis/</guid>
      <description>IntroductionThe data set is the mushroom set retrieved from: http://archive.ics.uci.edu/ml/datasets/Mushroom. It consists of 8124 orbs and 23 variables. The classification variable is the “type” either edible (e) or poisonous (p). The remaining 22 variables are the predictors and consist of multiple levels from 1 to 12 each. The data columns will need to be named and are coded in short for each. There are 2480 NA’s found and in one variable “sr” or stalk root.</description>
    </item>
    
    <item>
      <title>Random Forest and Decision Tree Analysis</title>
      <link>/post/2018/06/01/random-forest-and-decision-tree-analysis/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/06/01/random-forest-and-decision-tree-analysis/</guid>
      <description>Decision Tree and Random ForestThis analysis will utilize the data set from https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv and the purpose is to find the differences between the decision tree method and the random forest method for predicting wine quality from the 11 variables that indicate each wine’s chemical readings. The 12th variable is the quality rating given to these 1599 wines and is used as the predictor in the models. The quality has ratings from 3 to 8.</description>
    </item>
    
    <item>
      <title>Naive Bayes SMS Message Classifier</title>
      <link>/post/2018/05/27/naive-bayes-sms-message-classifier/</link>
      <pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/27/naive-bayes-sms-message-classifier/</guid>
      <description>Naive Bayes SMS Message ClassifierThis analysis demonstrates the naive bayes classifier in determining the spam or ham status of 4837 SMS messages. The data set is retrieved from: http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection. The data needs explicit cleaning to prepare it to run through a naive bayes classifier. A training and test set is created to train and validate the predictive ability ofy the model. There are summary plots of the words found in abundance in the data.</description>
    </item>
    
    <item>
      <title>K-Nearest Neighbor (KNN) - Heart Disease Dataset</title>
      <link>/post/2018/05/20/k-nearest-neighbor-knn-heart-disease-dataset/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/05/20/k-nearest-neighbor-knn-heart-disease-dataset/</guid>
      <description>Introduction to K-Nearest NeighborKNN is a supervised learning algorithm and uses a training sample from the dataset, which classifies groups into different classes. It is a classifier used to predict the level of an unknown point (observation) and does this by measuring the points nearest to the unknown point. It works well in measuring the differences between multiple classes that are complex and hard to detect. KNN is considered a simple classification and regression algorithm.</description>
    </item>
    
  </channel>
</rss>