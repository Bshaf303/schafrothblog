<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>EDA in Data Science on Bryan Schafroth Portfolio</title>
    <link>http://localhost:4321/tags/eda-in-data-science/</link>
    <description>Recent content in EDA in Data Science on Bryan Schafroth Portfolio</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 Aug 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:4321/tags/eda-in-data-science/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>K-Means Clustering Analysis</title>
      <link>http://localhost:4321/post/2018/08/12/k-means-clustering-analysis/</link>
      <pubDate>Sun, 12 Aug 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/08/12/k-means-clustering-analysis/</guid>
      <description>&lt;div id=&#34;k-means-clustering&#34; class=&#34;section level3&#34;&gt;&#xD;&#xA;&lt;h3&gt;K-Means Clustering&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;The first section uses the iris data set and k-means clustering starting with 3 cluster centers and an &lt;code&gt;nstart&lt;/code&gt; of 15. The results are displayed below in the fit object. There are 3 clusters sized 50, 62, and 38. The within sum of squares by cluster is 88.4% and the smaller number (15.15, 39.82, &amp;amp; 23.87) indicates how closely related objects are in the clusters. The first cluster has the most related objects and the second cluster has the lesser of related objects mostly taken from the third cluster.&lt;/p&gt;</description>
    </item>
    <item>
      <title>R Analysis - Plotting Graphics</title>
      <link>http://localhost:4321/post/2018/07/29/r-analysis-plotting-graphics/</link>
      <pubDate>Sun, 29 Jul 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/07/29/r-analysis-plotting-graphics/</guid>
      <description>&lt;div id=&#34;r-graphics-for-exploring-data&#34; class=&#34;section level3&#34;&gt;&#xD;&#xA;&lt;h3&gt;R Graphics for Exploring Data&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;The graphics device prints the graphics to screen or into a file type read by other apps. The screen device is on our computers. For windows its &lt;code&gt;windows()&lt;/code&gt;, mac its &lt;code&gt;quartz()&lt;/code&gt; and Linux its &lt;code&gt;x11()&lt;/code&gt;. I am assuming this is built into the R functions &lt;code&gt;plot()&lt;/code&gt;, &lt;code&gt;xyplot()&lt;/code&gt;, and &lt;code&gt;qplot()&lt;/code&gt; when we use them to plot. The other “devices” are the file types that are created by the device. PDF, PNG, JPEG, SVG file types are ways to plot graphics and view and share the plot. It depends where the plot is sent. Searching in RStudio I type: &lt;code&gt;?Devices&lt;/code&gt;, and get a List of Graphical Devices found in the &lt;code&gt;{grDevices}&lt;/code&gt; package that came loaded with RStudio. The available devices are windows, pdf, postscript, xfig, bitmap, and pictex. The other devices that may produce a warning message are: cairo.pdf, svg, png, jpeg, bmp, tiff. Some of these I recognize when I export a graph and save as image. If you are on Mac I would guess the first one would be the &lt;code&gt;quartz()&lt;/code&gt; and you wouldn’t see “windows” nor “x11” I would also presumption that with the 10,000+ packages for R there are more graphic devices available. There are two formats for the file devices which are vector and bitmap graphics. Vectors are more for line type graphics with simple color palettes, while bit maps are more like a very dense scatter plots or pictures that can handle color gradients and mixed colors. The &lt;code&gt;par()&lt;/code&gt; function sets the graphics device parameter in R and allowed plots to show up side by side and one on top of the other. &lt;code&gt;par()&lt;/code&gt; format stays on and will continuously show successive plots together side by side. The &lt;code&gt;dev.off()&lt;/code&gt; function is key here to reset the screen device parameters.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploratory Data Analysis - EPA Ozone Data</title>
      <link>http://localhost:4321/post/2018/07/14/exploratory-data-analysis-epa-ozone-data/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/07/14/exploratory-data-analysis-epa-ozone-data/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level4&#34;&gt;&#xD;&#xA;&lt;h4&gt;Introduction&lt;/h4&gt;&#xD;&#xA;&lt;p&gt;This exploratory data analysis will use the “hourly_44201_2014” data set obtained from the web site: &lt;a href=&#34;https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw&#34; class=&#34;uri&#34;&gt;https://aqs.epa.gov/aqsweb/airdata/download_files.html#Raw&lt;/a&gt; The data is “Criteria Gases” and labeled “Ozone (44201)” for this particular data set. The data is loaded into RStudio and shows 9,060,694 rows and 24 variables.&lt;/p&gt;&#xD;&#xA;&lt;p&gt;There are 24 variables in this data set. Looking through the variables, we can see the first four are coded presumably by the EPA. The Latitude, Longitude, and Datum are geo-location data. Datum NAD83 is the coordinate system for North American datum, while the WGS84 is a world coordinate system and tells that there are measurements taken presumably in another region or territory of the U.S. The “Date_Local” variable is clearly showing 365 unique dates, which cover the number of days in a year. The “Time_local” variable indicates 24 individual observations and covers each hour in a day. We see the variable measurements labeled as “Sample_Measurement” and then some more coded variables. The last variables are the “State_Name”, “County_Name”. The 53 individual names in the “State_Name” variable stand out, and this possibly coincides with the WGS84 coordinate system for U.S. States outside of the 48 States and are territories.&#xD;&#xA;In summary, we have essential geographic information, time/date information, and an Ozone measurement available in the data set. For this analysis, we can disregard the internal coding from the EPA.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Exploration - The Chicago Dataset</title>
      <link>http://localhost:4321/post/2018/07/07/data-exploration-the-chicago-dataset/</link>
      <pubDate>Sat, 07 Jul 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/07/07/data-exploration-the-chicago-dataset/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level4&#34;&gt;&#xD;&#xA;&lt;h4&gt;Introduction&lt;/h4&gt;&#xD;&#xA;&lt;p&gt;This article is about using the tidyverse package in R to explore data. The Chicago dataset is the basis for this analysis. The data is air quality measurements taken over 19 years. The focus is on using the dplyr package to do the analysis. The dplyr package is part of the tidyverse in R and a suite of tools to analyze, transform, plot, and manipulate data. The base version of R does all of this; however, dplyr will work faster and cost fewer computing resources when used on big data. The functions used to explore the data are as follows:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
