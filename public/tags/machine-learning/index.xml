<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Bryan Schafroth Portfolio</title>
    <link>http://localhost:4321/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Bryan Schafroth Portfolio</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 21 Oct 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:4321/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Classification Problem using Machine Learning</title>
      <link>http://localhost:4321/post/2018/10/21/classification-problem-using-machine-learning-forest-cover-type/</link>
      <pubDate>Sun, 21 Oct 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/10/21/classification-problem-using-machine-learning-forest-cover-type/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level4&#34;&gt;&#xD;&#xA;&lt;h4&gt;Introduction&lt;/h4&gt;&#xD;&#xA;&lt;p&gt;This article will present a machine learning problem and the concept behind solving the problem. The project will go into detail about the early exploratory steps and build a machine learning model that can be used in production to categorize data. This project is to categorize seven Rocky Mountain tree types found in the forests. In this project, we will use old legacy data from the 1990s and use modern machine learning methods to categorize the data. The owner of the data can migrate the newly categorized data over to their modern cloud database. In the late 1990s, data was collected for mapping by survey methods and done manually in the field. Though in the modern era, we can use high-resolution aerial imagery, GPS, and techniques in GIS systems to categorize cartographic data using the tools of computer vision and machine learning. However, for practical purposes, there are clients and owners of legacy data on old systems. Owners would like to migrate data from legacy systems into modern databases such as the cloud. The utilization of the data through modern computing is of use to the owners. One of the issues is how to categorize the vast amount of legacy data. Automating the categorization process with machine learning is the method proposed in this article.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reinforcement Learning Problem</title>
      <link>http://localhost:4321/post/2018/07/01/reinforcement-learing-problem/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/07/01/reinforcement-learing-problem/</guid>
      <description>&lt;div id=&#34;reinforcement-learning---grid-world-problem&#34; class=&#34;section level4&#34;&gt;&#xD;&#xA;&lt;h4&gt;Reinforcement Learning - Grid World Problem&lt;/h4&gt;&#xD;&#xA;&lt;p&gt;The first section looks at the grid world concept/problem. The environment is a 3x4 grid and the goal is to move from the start xy (1,1) (called a “state”&#34;) location to the opposite corner (4,3) called the goal state. The actions are up, down, left, right. We have the rewards of +1 &amp;amp; -1. We want the agent to find the shortest sequence of actions to get from start to end (goal). The additional rules are blocking the the state labeled by x=2/y=2 and avoiding the forfeiture state of x=4/y=2. Thus from the start state we can only go up or right. This is a delayed reward state concept because several moves need to be made before getting to the goal state.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K-Means - Hierarchial Cluster Analysis</title>
      <link>http://localhost:4321/post/2018/06/17/k-means-hierarchial-cluster-analysis/</link>
      <pubDate>Sun, 17 Jun 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/06/17/k-means-hierarchial-cluster-analysis/</guid>
      <description>&lt;div id=&#34;k-means-analysis&#34; class=&#34;section level3&#34;&gt;&#xD;&#xA;&lt;h3&gt;K-Means Analysis&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;This project will use the Wholesale customer Data Set from: &lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Wholesale+customers&#34; class=&#34;uri&#34;&gt;https://archive.ics.uci.edu/ml/datasets/Wholesale+customers&lt;/a&gt;. The data has 440 orbs and 8 variables. The data has the Channel and Region as integers but they are categorical in nature with 2 channels and 3 regions. We think this may need to be taken out of the analysis but will leave them in for now. A general observation about the variables there are outliers in the 6 major variables: fresh, milk, grocery, frozen, detergents_paper, delicassen. There are plot of each variable. In addition, to the outliers it is noted that the data is dense at the bottom of each of the variable graphs. “Fresh” seems to be less dense than the rest and “Delicassen” is the thickest density of points at the bottom. Initial thoughts are the clusters are going to be somewhere at the bottom of the graph.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Artificial Neural Network and Support Vector Machine Analysis</title>
      <link>http://localhost:4321/post/2018/06/10/artificial-neural-network-and-support-vector-machine-analysis/</link>
      <pubDate>Sun, 10 Jun 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/06/10/artificial-neural-network-and-support-vector-machine-analysis/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;&#xD;&#xA;&lt;h3&gt;Introduction&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;The data set is the mushroom set retrieved from: &lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/Mushroom&#34; class=&#34;uri&#34;&gt;http://archive.ics.uci.edu/ml/datasets/Mushroom&lt;/a&gt;. It consists of 8124 orbs and 23 variables. The classification variable is the “type” either edible (e) or poisonous (p). The remaining 22 variables are the predictors and consist of multiple levels from 1 to 12 each. The data columns will need to be named and are coded in short for each. There are 2480 NA’s found and in one variable “sr” or stalk root. It is decided to use kNN imputation (k=10) to fill these values in with one of the five levels found in this variable.The imputation works for categorical data and thus chosen for this task. It is decided to create dummy variables into numeric values (1,0), but first the veil-type “vt” only has one level and will be removed, also the imputation creates a sr_imp variable and this is also removed. There will be 117 variables, to reduce collinearity the 2 level variables are reduced to 1 variable (fullRank=T). The dependent variable “type” then added back to the revised set as a factor or 2 levels (e,p).&#xD;&#xA;The first section will analyze the ANN using the nnet() from the nnet package. The second section will analyze the SVM using the ksvm() from the kernlab package.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Random Forest and Decision Tree Analysis</title>
      <link>http://localhost:4321/post/2018/06/01/random-forest-and-decision-tree-analysis/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/06/01/random-forest-and-decision-tree-analysis/</guid>
      <description>&lt;div id=&#34;decision-tree-and-random-forest&#34; class=&#34;section level3&#34;&gt;&#xD;&#xA;&lt;h3&gt;Decision Tree and Random Forest&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;This analysis will utilize the data set from &lt;a href=&#34;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&#34; class=&#34;uri&#34;&gt;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&lt;/a&gt; and the purpose is to find the differences between the decision tree method and the random forest method for predicting wine quality from the 11 variables that indicate each wine’s chemical readings. The 12th variable is the quality rating given to these 1599 wines and is used as the predictor in the models. The quality has ratings from 3 to 8. To make the ratings work in the models there will be three groups “Fair” = (3-4), “Satisfactory” = (5-6), and “Excellent” = (7-8).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Naive Bayes SMS Message Classifier</title>
      <link>http://localhost:4321/post/2018/05/27/naive-bayes-sms-message-classifier/</link>
      <pubDate>Sun, 27 May 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/05/27/naive-bayes-sms-message-classifier/</guid>
      <description>&lt;div id=&#34;naive-bayes-sms-message-classifier&#34; class=&#34;section level3&#34;&gt;&#xD;&#xA;&lt;h3&gt;Naive Bayes SMS Message Classifier&lt;/h3&gt;&#xD;&#xA;&lt;p&gt;This analysis demonstrates the naive bayes classifier in determining the spam or ham status of 4837 SMS messages. The data set is retrieved from: &lt;a href=&#34;http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection&#34; class=&#34;uri&#34;&gt;http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection&lt;/a&gt;. The data needs explicit cleaning to prepare it to run through a naive bayes classifier. A training and test set is created to train and validate the predictive ability ofy the model. There are summary plots of the words found in abundance in the data. The summary will show the model’s predictive accuracy and the simplicity of running the data. Crossfold validation method is used and the Accuracy and Kappa are the metrics to judge performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K-Nearest Neighbor (KNN) - Heart Disease Dataset</title>
      <link>http://localhost:4321/post/2018/05/20/k-nearest-neighbor-knn-heart-disease-dataset/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/05/20/k-nearest-neighbor-knn-heart-disease-dataset/</guid>
      <description>&lt;div id=&#34;introduction-to-k-nearest-neighbor&#34; class=&#34;section level4&#34;&gt;&#xD;&#xA;&lt;h4&gt;Introduction to K-Nearest Neighbor&lt;/h4&gt;&#xD;&#xA;&lt;p&gt;KNN is a supervised learning algorithm and uses a training sample from the dataset, which classifies groups into different classes. It is a classifier used to predict the level of an unknown point (observation) and does this by measuring the points nearest to the unknown point. It works well in measuring the differences between multiple classes that are complex and hard to detect. KNN is considered a simple classification and regression algorithm. In classification, new data points are grouped into a given class, while in regression, a new data point is labeled based on the average value of k nearest neighbor. KNN is a “lazy learner” because it doesn’t learn more than the training data&lt;/p&gt;</description>
    </item>
    <item>
      <title>Real-Time Data and Smart Cities</title>
      <link>http://localhost:4321/post/2018/03/20/real-time-data-and-smart-cities/</link>
      <pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018/03/20/real-time-data-and-smart-cities/</guid>
      <description>&lt;h4 id=&#34;real-time-data-and-smart-cities&#34;&gt;&lt;strong&gt;Real-Time Data and Smart Cities&lt;/strong&gt;&lt;/h4&gt;&#xA;&lt;p&gt;    The Internet of Things (IoT) is a concept suggesting the number of devices connected to the Internet will continue to grow. The Internet has computers and mobile devices connected to it. The expectation is IoT will become more significant with the increasing use of sensors, actuators, and embedded devices. Currently, the Internet connects devices through wired and wireless technologies. Devices also can communicate over a network and then transfer the data to the Internet. According to Mahdavinejad et al. (2017), this technology is to make our activities and experiences easy and elevating.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
